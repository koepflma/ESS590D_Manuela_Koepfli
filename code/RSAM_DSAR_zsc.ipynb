{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e29a6a",
   "metadata": {},
   "source": [
    "# z-score normalization & csv files for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4243c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a21c4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "def zsc(s):\n",
    "    '''z-score normaliztion in log space (Ardid et al., 2022)'''\n",
    "    # log transform data\n",
    "    log_s = np.log10(s).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    # compute mean/std/min\n",
    "    mn = np.mean(log_s)\n",
    "    std = np.std(log_s)\n",
    "    minzsc = np.min(log_s)                                                    \n",
    "\n",
    "    # Calculate percentile\n",
    "    s=(np.log10(s)-mn)/std\n",
    "    s=s.fillna(minzsc)\n",
    "    s=10**s\n",
    "    return s\n",
    "\n",
    "def zsc2(s):\n",
    "    '''apply z-score normalization with rolling'''\n",
    "    s=zsc(s)\n",
    "    s=s.rolling(window=2).min()\n",
    "    s[0]=s[1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377a446",
   "metadata": {},
   "source": [
    "## Add z-score normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127dc5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JUN\n",
      "NED\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "SWFL\n",
      "TDL\n",
      "VALT\n",
      "***2010 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JUN\n",
      "NED\n",
      "SEP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koepflma/anaconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/koepflma/anaconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/koepflma/anaconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/koepflma/anaconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/koepflma/anaconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/koepflma/anaconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/home/koepflma/anaconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHW\n",
      "SOS\n",
      "STD\n",
      "SWFL\n",
      "TDL\n",
      "VALT\n",
      "***2011 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JUN\n",
      "NED\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "SWFL\n",
      "TDL\n",
      "VALT\n",
      "***2012 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JUN\n",
      "NED\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2013 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JRO\n",
      "JUN\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2014 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JRO\n",
      "JUN\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2015 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JRO\n",
      "JUN\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2016 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JRO\n",
      "JUN\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2017 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JRO\n",
      "JUN\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2018 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JRO\n",
      "JUN\n",
      "REM\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2019 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HSR\n",
      "JRO\n",
      "REM\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2020 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HOA\n",
      "HSR\n",
      "LOO\n",
      "REM\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "VALT\n",
      "***2021 done***\n",
      "CDF\n",
      "EDM\n",
      "ELK\n",
      "FL2\n",
      "HOA\n",
      "HSR\n",
      "LOO\n",
      "REM\n",
      "SEP\n",
      "SHW\n",
      "SOS\n",
      "STD\n",
      "TDL\n",
      "USFR\n",
      "VALT\n",
      "***2022 done***\n"
     ]
    }
   ],
   "source": [
    "# year = 2009\n",
    "#============================================================================================\n",
    "\n",
    "for year in range(2010,2022+1):\n",
    "    # calculate z-score normalization and save one file per station\n",
    "    all_files = sorted(glob.glob('/data/wsd03/data_manuela/MtStHelens/RSAM_DSAR/tmp_{}/*/*.csv'.format(year)))\n",
    "    sta_list = np.unique([file.split('/')[-2] for file in all_files]) # station list based on the files\n",
    "    for sta in sta_list:\n",
    "        print(sta)\n",
    "        # all days (one year) of one station\n",
    "        sta_files = sorted(glob.glob('/data/wsd03/data_manuela/MtStHelens/RSAM_DSAR/tmp_{}/{}/*.csv'.format(year,sta)))\n",
    "        li = [] # empty list\n",
    "        for filename in sta_files:\n",
    "            frame = pd.read_csv(filename) # read one day\n",
    "            li.append(frame) # all days (one year) of one station\n",
    "\n",
    "        df = pd.concat(li, axis=0, ignore_index=True) # list to df\n",
    "        df.set_index('time',inplace=True) # index to time\n",
    "        df.index = pd.to_datetime(df.index).tz_localize(None) # time to datetime\n",
    "        df.sort_index() # sort by date (index)\n",
    "\n",
    "        # compute transform for each column (time series)\n",
    "        df['zsc2_rsam'] = zsc2(df['rsam'])\n",
    "        df['zsc2_mf'] = zsc2(df['mf'])\n",
    "        df['zsc2_hf'] = zsc2(df['hf'])\n",
    "        df['zsc2_dsar'] = zsc2(df['dsar'])\n",
    "        df['zsc2_ldsar'] = zsc2(df['ldsar'])\n",
    "        df['zsc2_vsar'] = zsc2(df['vsar'])\n",
    "        df['zsc2_rms'] = zsc2(df['rms'])\n",
    "        df['zsc2_rmes'] = zsc2(df['rmes'])\n",
    "        df['zsc2_pgv'] = zsc2(df['pgv'])\n",
    "        df['zsc2_pga'] = zsc2(df['pga'])\n",
    "\n",
    "        df.to_csv('/home/koepflma/project1/Mt-St-Helens/RSAM_DSAR/data/{}_{}_extended2.csv'.format(year,sta)) # save csv file with z-score normalization\n",
    "    #     df.to_csv('../{}_{}_extended2.csv'.format(year,sta)) # save csv file with z-score normalization\n",
    "    print('***{} done***'.format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e05f37",
   "metadata": {},
   "source": [
    "## Create multi-year long files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "288ea395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000_SEP_extended2.csv', '2001_SEP_extended2.csv', '2002_SEP_extended2.csv', '2003_SEP_extended2.csv', '2004_SEP_extended2.csv', '2005_SEP_extended2.csv', '2006_SEP_extended2.csv', '2007_SEP_extended2.csv', '2008_SEP_extended2.csv', '2009_SEP_extended2.csv', '2010_SEP_extended2.csv', '2011_SEP_extended2.csv', '2012_SEP_extended2.csv', '2013_SEP_extended2.csv', '2014_SEP_extended2.csv', '2015_SEP_extended2.csv', '2016_SEP_extended2.csv', '2017_SEP_extended2.csv', '2018_SEP_extended2.csv', '2019_SEP_extended2.csv', '2020_SEP_extended2.csv', '2021_SEP_extended2.csv', '2022_SEP_extended2.csv']\n",
      "***done***\n"
     ]
    }
   ],
   "source": [
    "sta = 'SEP'\n",
    "#============================================================================================\n",
    "# notice, you already need the extended2.csv file for the station\n",
    "all_files = sorted(glob.glob('/home/koepflma/project1/Mt-St-Helens/RSAM_DSAR/data/*_{}_extended2.csv'.format(sta)))\n",
    "print([file.split('/')[-1] for file in all_files]) # check how many years you have\n",
    "li = [] # empty list\n",
    "for filename in all_files:\n",
    "    frame = pd.read_csv(filename) # read one year\n",
    "    li.append(frame) # all years of one station\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True) # list to df\n",
    "df.set_index('time',inplace=True) # index to time\n",
    "df.index = pd.to_datetime(df.index).tz_localize(None) # time to datetime\n",
    "df.sort_index() # sort by date (index)\n",
    "df.to_csv('/home/koepflma/project1/Mt-St-Helens/RSAM_DSAR/data/{}_extended2_long2.csv'.format(sta)) # save csv file with multiple years of one station\n",
    "print('***done***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a67dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
